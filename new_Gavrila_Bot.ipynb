{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Бот Гаврила Новый"
      ],
      "metadata": {
        "id": "lqA3nUw8Djkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Общая инструкция по созданию и работе с телеграм ботом\n",
        "Шаг 1. Регистрируем бота\n",
        "\n",
        "Находим в поиске Телеграма - бота @botFather — это главный инструмент для создания ботов. Набираем в нём /newbot.\n",
        "Придумываем название и имя боту. Название — то, как он будет подписан в списке контактов. Имя — строка, по которой его можно будет найти. Обратите внимание, что имя должно оканчиваться на bot. В ответ мы получим токен — длинную последовательность символов, которая пригодится нам в будущем. Мы назвали бота echoKeksBot, но вам придётся придумать другое название, потому что это уже занято. Извините.\n",
        "\n",
        "Шаг 2.\n",
        "\n",
        "в коде самого ботанаходим строку кода - \"token = 'ваш_токен'\" и подставляем туда ваш токен.\n",
        "\n",
        "Шаг 3.\n",
        "\n",
        "Последовательно запускаем ячейки данного ноутбука\n",
        "\n",
        "(Код самого бота находится в конце этой тетрадки, именно там нужно записать свой токен в строку кода \"token = 'ваш_токен'\"\n",
        "Запуская бота в гугл колабе мы заставляем ячейку с кодом бота выполнять бесконечный цикл. Поэтому ваш бот будет работать пока длится сессия в гугл колабе.)\n",
        "\n",
        "Шаг 4.\n",
        "\n",
        "Переходим в телеграм, открываем вашего бота, вводим команду /start\n",
        "\n",
        "Шаг 5.\n",
        "\n",
        "Общаемся с ботом\n",
        "\n",
        "Останавливаем бот повторным нажатием на ячейку с кодом бота.\n"
      ],
      "metadata": {
        "id": "dH1jcTzVq2Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Бот в этом ноутбуке имеет следующий функционал:\n",
        "\n",
        "1) чат с LLM - llama-7B-hf (на английском и русском языках) взято с https://huggingface.co/baffo32/decapoda-research-llama-7B-hf\n",
        "\n",
        "2)чат с chatgpt 3.5 (используется api сайта openai.com)\n",
        "\n",
        "3)предсказание температуры воздуха в Москве с выводом графика (интервал прогнозирования с 11.11.2023 до 09.01.2024г.) Сами спронозированные значения температуры находятся здесь: https://drive.google.com/file/d/15Yxkm93tTsgStHPcUU2W5W_yv3GQyuDI/view?usp=sharing\n",
        "\n",
        "4)Вывод значений температуры воздуха в любом городе на текущую дату (используется api сайта openweathermap.org)"
      ],
      "metadata": {
        "id": "fGnt3m-trRlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Подключаем библиотеки для LLM"
      ],
      "metadata": {
        "id": "wz-DdykgnsCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmxWd1lAixyS",
        "outputId": "9050c9aa-6f4a-4458-c838-b37f7bb29c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.2.post2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10g5o1tFizOq",
        "outputId": "1bbf6d7c-0477-4f40-a12b-c83bba152552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.27.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Скачиваем модель и токенизатор"
      ],
      "metadata": {
        "id": "S3rvZsUCn-cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"baffo32/decapoda-research-llama-7B-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "EY-DJ2SUi7EF",
        "outputId": "2ad93628-8ab7-4f1c-b90b-bd5938cea126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
            "The class this function is called from is 'LLaMATokenizer'.\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9e629b163677>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLaMATokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"baffo32/decapoda-research-llama-7B-hf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = LLaMAForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"baffo32/decapoda-research-llama-7B-hf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m                 }\n\u001b[1;32m   2577\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   2579\u001b[0m                         \"\"\"\n\u001b[1;32m   2580\u001b[0m                         \u001b[0mSome\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdispatched\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCPU\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMake\u001b[0m \u001b[0msure\u001b[0m \u001b[0myou\u001b[0m \u001b[0mhave\u001b[0m \u001b[0menough\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mRAM\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you have set a value for `max_memory` you should increase that. To have\n                        an idea of the modules that are set on the CPU or RAM you can print model.hf_device_map.\n                        "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Устанавливаем переводчик для перевода ответов модели на русский"
      ],
      "metadata": {
        "id": "SBs7uodioJf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4jRXK-PjGr-",
        "outputId": "b6c004b6-9cd1-40aa-c90e-f4a303f975a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator"
      ],
      "metadata": {
        "id": "dHGcbqyrjQ0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Функции перевода и получения ответа модели на русском языке"
      ],
      "metadata": {
        "id": "4iq-UDIZoesT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "\n",
        "def translate_text(text):\n",
        "    translation = translator.translate(text, src='ru', dest='en')\n",
        "    return translation.text\n",
        "\n",
        "def translate_text_ru(text):\n",
        "    translation = translator.translate(text, src='en', dest='ru')\n",
        "    return translation.text\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    instruction_english = translate_text(instruction)\n",
        "    if input:\n",
        "        input_english = translate_text(input)\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction_english}\n",
        "\n",
        "### Input:\n",
        "{input_english}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction_english}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    )\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    instruction_english = translate_text(instruction)\n",
        "    input_english = None\n",
        "    if input:\n",
        "        input_english = translate_text(input)\n",
        "    prompt = generate_prompt(instruction_english, input_english)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=50\n",
        "    )\n",
        "    responses = []\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        if \"### Response:\" in output:\n",
        "            response_start = output.index(\"### Response:\") + len(\"### Response:\")\n",
        "            response_end = output.index(\"\\n\\n\", response_start) if \"\\n\\n\" in output[response_start:] else None\n",
        "            response = output[response_start:response_end].strip()  # get text after \"### Response:\" and before \"\\n\\n\"\n",
        "            responses.append(response)\n",
        "            output_russian = translate_text_ru(output.split(\"### Response:\")[1].strip())\n",
        "        return output_russian"
      ],
      "metadata": {
        "id": "eozfwcqTjb3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Функция получения ответа на родном языке модели (английский)"
      ],
      "metadata": {
        "id": "_0jGq52Jov7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate1(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=50\n",
        "    )\n",
        "    responses = []\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        if \"### Response:\" in output:\n",
        "            response_start = output.index(\"### Response:\") + len(\"### Response:\")\n",
        "            response_end = output.index(\"\\n\\n\", response_start) if \"\\n\\n\" in output[response_start:] else None\n",
        "            response = output[response_start:response_end].strip()  # get text after \"### Response:\" and before \"\\n\\n\"\n",
        "            responses.append(response)\n",
        "    return responses[0]"
      ],
      "metadata": {
        "id": "IFiAYOJdjo9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Функцция для получения ответа от chatgpt 3.5"
      ],
      "metadata": {
        "id": "kWU_2yUSo_Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_chatbot_response(query):\n",
        "    # Ключ из личного кабинета, подставьте свой\n",
        "    CHAD_API_KEY = 'Здесь ваш ключ'\n",
        "\n",
        "    # Формируем запрос\n",
        "    request_json = {\n",
        "        \"message\": query,\n",
        "        \"api_key\": CHAD_API_KEY,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Отправляем запрос и дожидаемся ответа\n",
        "        response = requests.post(url='https://ask.chadgpt.ru/api/public/gpt-3.5',\n",
        "                                 json=request_json)\n",
        "\n",
        "        # Проверяем, отправился ли запрос\n",
        "        if response.status_code != 200:\n",
        "            return f'Ошибка! Код http-ответа: {response.status_code}'\n",
        "        else:\n",
        "            # Получаем текст ответа и преобразовываем в dict\n",
        "            resp_json = response.json()\n",
        "\n",
        "            # Если успешен ответ, то выводим\n",
        "            if resp_json['is_success']:\n",
        "                resp_msg = resp_json['response']\n",
        "                used_words = resp_json['used_words_count']\n",
        "                return f'Ответ от бота: {resp_msg}'\n",
        "            else:\n",
        "                error = resp_json['error_message']\n",
        "                return f'Ошибка: {error}'\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print (f\"Возникла ошибка: {err}\")"
      ],
      "metadata": {
        "id": "ZT9OlgcHRfAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Устанавливаем api Telegram"
      ],
      "metadata": {
        "id": "ONfXCNbWpQYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytelegrambotapi"
      ],
      "metadata": {
        "id": "Y4snlHXEDrua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0e75d3-73be-471c-f4b9-5d79b27e4324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytelegrambotapi\n",
            "  Downloading pyTelegramBotAPI-4.14.0.tar.gz (243 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/243.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/243.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m204.8/243.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.1/243.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytelegrambotapi) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (2023.7.22)\n",
            "Building wheels for collected packages: pytelegrambotapi\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-4.14.0-py3-none-any.whl size=215250 sha256=e2c777fe3293954af52471d098893aaa4be4889ba89faecedfb73ee44c96c61d\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/51/2d/24b40a366c85c37928d5aa36ddf257e5a79fad25e1ecd11b2c\n",
            "Successfully built pytelegrambotapi\n",
            "Installing collected packages: pytelegrambotapi\n",
            "Successfully installed pytelegrambotapi-4.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Импортируем библиотеки"
      ],
      "metadata": {
        "id": "31sYR05zphTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "b0qwe1h6D0O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io"
      ],
      "metadata": {
        "id": "HfUPcHkpD3zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Внимание!!! Перед запуском бота нужно загрузить в сессионное хранилище файл с прогнозными значениями - \"df_all.csv\"\n",
        "\n",
        "https://drive.google.com/uc?id=15Yxkm93tTsgStHPcUU2W5W_yv3GQyuDI"
      ],
      "metadata": {
        "id": "WMCJn1mHWe4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 15Yxkm93tTsgStHPcUU2W5W_yv3GQyuDI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP951Rwdt55L",
        "outputId": "3e47ace6-f101-4b3e-a665-2536f108816b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15Yxkm93tTsgStHPcUU2W5W_yv3GQyuDI\n",
            "To: /content/df_all.csv\n",
            "\r  0% 0.00/5.15k [00:00<?, ?B/s]\r100% 5.15k/5.15k [00:00<00:00, 23.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Код самого бота"
      ],
      "metadata": {
        "id": "BZZZ3HBwpoNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot\n",
        "from telebot import types\n",
        "\n",
        "token = 'Здесь ваш токен'\n",
        "bot = telebot.TeleBot(token)\n",
        "\n",
        "date_requested = False  # Переменная для отслеживания режима предсказания погоды\n",
        "ctrl_button = False\n",
        "lora_eng = False\n",
        "lora_rus = False\n",
        "gpt_ok = False\n",
        "forecast_requested = False\n",
        "img_ok = True\n",
        "\n",
        "# Главное меню с кнопками\n",
        "@bot.message_handler(commands=['start'])\n",
        "def handle_start(message):\n",
        "    markup = types.ReplyKeyboardMarkup(row_width=2)\n",
        "    itembtn1 = types.KeyboardButton('погода')\n",
        "    itembtn2 = types.KeyboardButton('Вопрос LLM')\n",
        "    markup.add(itembtn1, itembtn2)\n",
        "    bot.send_message(chat_id=message.chat.id, text=\"Выберите один из вариантов:\", reply_markup=markup)\n",
        "\n",
        "\n",
        "# Обработчик кнопки \"погода\"\n",
        "@bot.message_handler(func=lambda message: message.text == 'погода')\n",
        "def handle_weather(message):\n",
        "    markup = types.InlineKeyboardMarkup(row_width=2)\n",
        "    itembtn1 = types.InlineKeyboardButton('сегодня', callback_data='today')\n",
        "    itembtn2 = types.InlineKeyboardButton('прогноз для Москвы', callback_data='forecast')\n",
        "    markup.add(itembtn1, itembtn2)\n",
        "    global ctrl_button\n",
        "    ctrl_button=True\n",
        "    bot.send_message(chat_id=message.chat.id, text=\"Выберите один из вариантов:\", reply_markup=markup)\n",
        "\n",
        "# Обработчик кнопки \"Вопрос LLM\"\n",
        "@bot.message_handler(func=lambda message: message.text == 'Вопрос LLM')\n",
        "def handle_lora(message):\n",
        "    markup = types.InlineKeyboardMarkup(row_width=2)\n",
        "    itembtn1 = types.InlineKeyboardButton('LLM русский', callback_data='russian')\n",
        "    itembtn2 = types.InlineKeyboardButton('LLM английский', callback_data='english')\n",
        "    itembtn3 = types.InlineKeyboardButton('gpt-3.5', callback_data='gpt')\n",
        "    markup.add(itembtn1, itembtn2, itembtn3)\n",
        "    global ctrl_button\n",
        "    ctrl_button=True\n",
        "    bot.send_message(chat_id=message.chat.id, text=\"Выберите язык:\", reply_markup=markup)\n",
        "\n",
        "\n",
        "# Обработчик нажатий инлайн-кнопок\n",
        "@bot.callback_query_handler(func=lambda call: True)\n",
        "def handle_inline_buttons(call):\n",
        "    if call.data == 'today':\n",
        "        bot.send_message(chat_id=call.message.chat.id, text=\"Введите название города\")\n",
        "        global date_requested\n",
        "        global forecast_requested\n",
        "        date_requested = False\n",
        "        forecast_requested = True\n",
        "    elif call.data == 'forecast':\n",
        "        bot.send_message(chat_id=call.message.chat.id, text=\"Введите дату для прогноза погоды в Москве \\n (в формате YYYY-MM-DD)\")\n",
        "        #global date_requested\n",
        "        date_requested = True\n",
        "        forecast_requested = False\n",
        "    elif call.data == 'russian':\n",
        "        bot.send_message(chat_id=call.message.chat.id, text=\"Привет, я - LLM!\\n Общайтесь со мной на русском, но умоляю - не заставляйте писать код!\")\n",
        "        global lora_eng\n",
        "        global lora_rus\n",
        "        lora_eng = False\n",
        "        lora_rus = True\n",
        "        date_requested = False\n",
        "        forecast_requested = False\n",
        "    elif call.data == 'english':\n",
        "        bot.send_message(chat_id=call.message.chat.id, text=\"Hello, I'm LLM! Ask me anything\")\n",
        "        lora_eng = True\n",
        "        lora_rus = False\n",
        "        date_requested = False\n",
        "        forecast_requested = False\n",
        "    elif call.data == 'gpt':\n",
        "        bot.send_message(chat_id=call.message.chat.id, text=\"Привет, я chatgpt! Спроси меня о чем нибудь!\")\n",
        "        lora_eng = False\n",
        "        lora_rus = False\n",
        "        date_requested = False\n",
        "        forecast_requested = False\n",
        "        global gpt_ok\n",
        "        gpt_ok = True\n",
        "    else:\n",
        "        bot.send_message(chat_id=call.message.chat.id, text=\"Такого варианта нет\")\n",
        "\n",
        "# Обработчик ввода названия города\n",
        "@bot.message_handler(func=lambda message: forecast_requested == True and ctrl_button==True)\n",
        "def handle_city(message):\n",
        "    city = message.text\n",
        "    o=f1(city)  # Вызов функции для получения температуры воздуха в указанном городе\n",
        "    bot.send_message(chat_id=message.chat.id, text=o)\n",
        "\n",
        "# Обработчик ввода даты для прогноза погоды\n",
        "@bot.message_handler(func=lambda message: date_requested == True and ctrl_button==True)\n",
        "def handle_date(message):\n",
        "    date = message.text\n",
        "    o1=f2(date)  # Вызов функции для отображения графика погоды на указанную дату\n",
        "    #bot.send_message(chat_id=message.chat.id, text=o1)\n",
        "    global img_ok\n",
        "    if img_ok == True:\n",
        "        with open('forecast.png', 'rb') as photo:\n",
        "            bot.send_photo(chat_id=message.chat.id, photo=photo)\n",
        "    bot.send_message(chat_id=message.chat.id, text=o1)\n",
        "\n",
        "# Обработчик ввода сообщения для Лоры английской\n",
        "@bot.message_handler(func=lambda message: lora_eng == True and ctrl_button==True)\n",
        "def process_message_eng(message):\n",
        "    user_message_eng = message.text\n",
        "    answer1 = lora_eng_f(user_message_eng)\n",
        "    bot.send_message(chat_id=message.chat.id, text=answer1)\n",
        "\n",
        "# Обработчик ввода сообщения для Лоры русской\n",
        "@bot.message_handler(func=lambda message: lora_rus == True and ctrl_button==True)\n",
        "def process_message_rus(message):\n",
        "    user_message_rus = message.text\n",
        "    answer2 = lora_rus_f(user_message_rus)\n",
        "    bot.send_message(chat_id=message.chat.id, text=answer2)\n",
        "\n",
        "# Обработчик ввода сообщения для chatgpt\n",
        "@bot.message_handler(func=lambda message: gpt_ok == True and ctrl_button==True)\n",
        "def process_message_gpt(message):\n",
        "    user_message_gpt = message.text\n",
        "    answer2 = fetch_chatbot_response(user_message_gpt)\n",
        "    bot.send_message(chat_id=message.chat.id, text=answer2)\n",
        "\n",
        "# Обработчик всех остальных текстовых сообщений\n",
        "@bot.message_handler(func=lambda m: True)  # этот обработчик должен идти последним\n",
        "def echo_all(message):\n",
        "    bot.reply_to(message, 'Я не понимаю')\n",
        "\n",
        "# Функция для обработки сообщений пользователя Лорой английской\n",
        "def lora_eng_f(message):\n",
        "    # Мой код для обработки сообщений пользователя Лорой\n",
        "    result = evaluate1(message)\n",
        "    #result =\"Hello, i'm Lora\"\n",
        "    return result\n",
        "\n",
        "# Функция для обработки сообщений пользователя Лорой русской\n",
        "def lora_rus_f(message):\n",
        "    # Мой код для обработки сообщений пользователя Лорой\n",
        "    result = evaluate(message)\n",
        "    #result =\"Привет, я Лора\"\n",
        "    return result\n",
        "\n",
        "# Функция для получения температуры воздуха в указанном городе\n",
        "def f1(city):\n",
        "    import requests\n",
        "    url = 'https://api.openweathermap.org/data/2.5/weather?q='+city+'&units=metric&lang=ru&appid=79d1ca96933b0328e1c7e3e7a26cb347'\n",
        "\n",
        "    try:\n",
        "        # Отправляем запрос на сервер и получаем результат\n",
        "        weather_data = requests.get(url).json()\n",
        "        # Получаем данные о температуре и ощущается ли она\n",
        "        temperature = round(weather_data['main']['temp'])\n",
        "        temperature_feels = round(weather_data['main']['feels_like'])\n",
        "        return f\"Температура в городе {city} сейчас {str(temperature)} °C \\n Ощущается как {str(temperature_feels)} °C.\"\n",
        "    except:\n",
        "        return \"Города не существует\"\n",
        "\n",
        "# Функция для отображения графика погоды на указанную дату\n",
        "def f2(date):\n",
        "    global img_ok\n",
        "    # Здесь должна быть логика получения графика погоды на указанную дату\n",
        "    from datetime import datetime\n",
        "    # Проверяем, что дата в правильном формате\n",
        "    try:\n",
        "       valid_date = datetime.strptime(date, '%Y-%m-%d')\n",
        "    except ValueError:\n",
        "        img_ok = False\n",
        "        return(\"Неверный формат даты. Пожалуйста, введите дату в формате YYYY-MM-DD\")\n",
        "    df_all = pd.read_csv('df_all.csv')\n",
        "    df_all['date'] = pd.to_datetime(df_all['date'])\n",
        "    # Проверяем, что дата находится в диапазоне дат в DataFrame\n",
        "    if 'date' in df_all:\n",
        "        min_date = df_all['date'].min()\n",
        "        max_date = df_all['date'].max()\n",
        "\n",
        "        if valid_date < min_date or valid_date > max_date:\n",
        "            img_ok = False\n",
        "            return(\"Введенная дата выходит за границы  периода прогноза.\")\n",
        "    # Фильтруем исходный DataFrame по дате\n",
        "    date_filter = df_all['date'] >= pd.to_datetime(date)\n",
        "    df_filtered = df_all.loc[date_filter]\n",
        "\n",
        "    plt.figure(figsize=(20, 6))\n",
        "\n",
        "    # Строим графики с использованием отфильтрованного DataFrame\n",
        "    plt.plot(df_filtered['date'], df_filtered['y_lstm'], label='Прогноз LSTM')\n",
        "    plt.plot(df_filtered['date'], df_filtered['y_prophet2'], label='Прогноз Prophet')\n",
        "    plt.plot(df_filtered['date'], df_filtered['y_fourier2'], label='Прогноз Fourier')\n",
        "    plt.plot(df_filtered['date'], df_filtered['y_mean'], label='Среднее значение')\n",
        "\n",
        "    # Указываем названия осей и заголовок\n",
        "    plt.title('Прогноз Температуры воздуха начиная с ' + date)\n",
        "    plt.xlabel('Дата')\n",
        "    plt.ylabel('Температура')\n",
        "\n",
        "    # Конвертация времени для удобного отображения на графике\n",
        "    plt.gcf().autofmt_xdate()\n",
        "\n",
        "    # Показываем легенду\n",
        "    plt.legend()\n",
        "\n",
        "    # Сохранение графика в файл\n",
        "    plt.savefig('forecast.png')\n",
        "    plt.close()\n",
        "    #plt.show()\n",
        "    val_lstm = df_all[df_all['date'] == date]['y_lstm'].values[0]\n",
        "    val_prophet = df_all[df_all['date'] == date]['y_prophet2'].values[0]\n",
        "    val_fourier = df_all[df_all['date'] == date]['y_fourier2'].values[0]\n",
        "    val_mean = df_all[df_all['date'] == date]['y_mean'].values[0]\n",
        "    # Например: bot.send_photo(chat_id=call.message.chat.id, photo=open('path_to_image', 'rb'))\n",
        "    # Ваш код для предсказания погоды на указанную дату\n",
        "    # Отправка сообщения с текстом и изображением\n",
        "    img_ok = True\n",
        "    return (\n",
        "    f'Прогноз от LSTM на {date}\\n температура воздуха составит {val_lstm:.1f} °C.\\n\\n'\n",
        "    f'Прогноз от prophet на {date}\\n температура воздуха составит {val_prophet:.1f} °C.\\n\\n'\n",
        "    f'Прогноз от Fourier на {date}\\n температура воздуха составит {val_fourier:.1f} °C.\\n\\n'\n",
        "    f'Прогноз от mean на {date}\\n температура воздуха составит {val_mean:.1f} °C.')\n",
        "\n",
        "\n",
        "bot.polling()"
      ],
      "metadata": {
        "id": "qbtg_RW1D8oq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
